{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def find_answer(question, context):\n",
    "    # Preprocess the context\n",
    "    context_tokens = preprocess_text(context)\n",
    "\n",
    "    # Preprocess the question\n",
    "    question_tokens = preprocess_text(question)\n",
    "\n",
    "    # Create a dictionary to store the relevance of each sentence\n",
    "    sentence_relevance = defaultdict(int)\n",
    "\n",
    "    # Iterate through the sentences in the context\n",
    "    for sentence in sent_tokenize(context):\n",
    "        sentence_tokens = preprocess_text(sentence)\n",
    "        for token in question_tokens:\n",
    "            if token in sentence_tokens:\n",
    "                sentence_relevance[sentence] += 1\n",
    "\n",
    "    # Find the most relevant sentence\n",
    "    most_relevant_sentence = max(sentence_relevance, key=sentence_relevance.get)\n",
    "\n",
    "    # Return the most relevant sentence as the answer\n",
    "    return most_relevant_sentence\n",
    "\n",
    "# Load the SQuAD dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('squad')\n",
    "\n",
    "# Extract the context from the dataset\n",
    "context = dataset['train'][0]['context']\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    answer = find_answer(user_input, context)\n",
    "    print(\"Chatbot:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
